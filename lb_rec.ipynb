{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1inhfSXexp8vJ2cg_iupjHrZWmbDWyzSF",
      "authorship_tag": "ABX9TyNvQ3OpqnPsOFVIMm86U29X",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AnsarYesma/letterboxd_watchlist_picker/blob/main/lb_rec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EUNYh08gy2UL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.sparse import csr_matrix\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import requests\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
        "\n",
        "!pip install bs4 aiohttp\n",
        "\n",
        "def load_data(file_path):\n",
        "    df = pd.read_csv(file_path)\n",
        "    return df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cMglT37phibx",
        "outputId": "64656b3b-cb16-4246-e01b-29b2bf12606b"
      },
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: bs4 in /usr/local/lib/python3.11/dist-packages (0.0.2)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (3.11.14)\n",
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (from bs4) (4.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (6.2.0)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (0.3.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp) (1.18.3)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.10)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4->bs4) (4.12.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Applied Z-score normalisation"
      ],
      "metadata": {
        "id": "9OP0R0buGigX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(df):\n",
        "    df = df.rename(columns={\"user_id\": \"uid\", \"movie_id\": \"fid\", \"rating_val\": \"rating\"})\n",
        "    df[\"rating\"] = (df[\"rating\"] - df[\"rating\"].mean()) / df[\"rating\"].std()\n",
        "\n",
        "    uid_mapping = {uid: i for i, uid in enumerate(df[\"uid\"].unique())}\n",
        "    fid_mapping = {fid: i for i, fid in enumerate(df[\"fid\"].unique())}\n",
        "\n",
        "    df[\"uid_index\"] = df[\"uid\"].map(uid_mapping)\n",
        "    df[\"fid_index\"] = df[\"fid\"].map(fid_mapping)\n",
        "    return df, uid_mapping, fid_mapping"
      ],
      "metadata": {
        "id": "cnh8wg2YhqJx"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The dataset is sparse so we use csr"
      ],
      "metadata": {
        "id": "Lr8WOEdtG__7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_matrix(df, num_users, num_films):\n",
        "    matrix = csr_matrix(\n",
        "        (df[\"rating\"], (df[\"uid_index\"], df[\"fid_index\"])),\n",
        "        shape=(num_users, num_films)\n",
        "    )\n",
        "    return matrix"
      ],
      "metadata": {
        "id": "lNlzEsffhuCa"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "SVD is often used for collaborative filtering"
      ],
      "metadata": {
        "id": "l2kJswHDHqbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_svd(matrix, n_components=50):\n",
        "    svd = TruncatedSVD(n_components=n_components)\n",
        "    latent_matrix = svd.fit_transform(matrix)\n",
        "    return svd, latent_matrix"
      ],
      "metadata": {
        "id": "9BapG2adhz8a"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part deals with functions responsible for scraping data about the user."
      ],
      "metadata": {
        "id": "J8MPB8HvH1Fp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "import aiohttp\n",
        "import asyncio\n",
        "\n",
        "def scrape_letterboxd_diary(username):\n",
        "    url = f\"https://letterboxd.com/{username}/films/diary/\"\n",
        "    response = requests.get(url)\n",
        "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "\n",
        "    films = []\n",
        "    for entry in soup.select(\"tr.diary-entry-row\"):\n",
        "        title_tag = entry.select_one(\".headline-3 a\")\n",
        "        fid = title_tag[\"href\"].split(\"/\")[-2] if title_tag and title_tag[\"href\"] else None\n",
        "\n",
        "        rating_tag = entry.select_one(\".td-rating input\")\n",
        "        rating = rating_tag['value'] if rating_tag else None\n",
        "\n",
        "        if fid and rating:\n",
        "          films.append({\"fid\": fid, \"rating\": rating, \"uid\": username})\n",
        "\n",
        "    return films[:10]\n",
        "\n",
        "import aiohttp\n",
        "import asyncio\n",
        "\n",
        "async def fetch_page(session, url):\n",
        "    async with session.get(url) as response:\n",
        "        return await response.text()\n",
        "\n",
        "async def scrape_watched(username):\n",
        "    all_fids = []\n",
        "    base_url = f\"https://letterboxd.com/{username}/films/\"\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        # Get total pages\n",
        "        first_page_html = await fetch_page(session, base_url)\n",
        "        soup = BeautifulSoup(first_page_html, \"html.parser\")\n",
        "\n",
        "        try:\n",
        "            page_count = int(soup.select(\"li.paginate-page\")[-1].text) + 1\n",
        "        except IndexError:\n",
        "            page_count = 2\n",
        "\n",
        "        tasks = [fetch_page(session, f\"{base_url}page/{page}/\") for page in range(1, page_count)]\n",
        "        pages_html = await asyncio.gather(*tasks)\n",
        "\n",
        "        for html in pages_html:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            fids = [li.div[\"data-film-slug\"] for li in soup.select(\"li.poster-container\")]\n",
        "            all_fids.extend(fids)\n",
        "\n",
        "    return all_fids\n",
        "\n",
        "async def scrape_watchlist(username):\n",
        "    all_fids = []\n",
        "    base_url = f\"https://letterboxd.com/{username}/watchlist/\"\n",
        "\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        first_page_html = await fetch_page(session, base_url)\n",
        "        soup = BeautifulSoup(first_page_html, \"html.parser\")\n",
        "\n",
        "        try:\n",
        "            page_count = int(soup.select(\"li.paginate-page\")[-1].text) + 1\n",
        "        except IndexError:\n",
        "            page_count = 2\n",
        "\n",
        "        tasks = [fetch_page(session, f\"{base_url}page/{page}/\") for page in range(1, page_count)]\n",
        "        pages_html = await asyncio.gather(*tasks)\n",
        "\n",
        "        for html in pages_html:\n",
        "            soup = BeautifulSoup(html, \"html.parser\")\n",
        "            fids = [li.div[\"data-film-slug\"] for li in soup.select(\"li.poster-container\")]\n",
        "            all_fids.extend(fids)\n",
        "\n",
        "    return all_fids\n",
        "\n",
        "async def scrap_lb(username):\n",
        "  last_20 = scrape_letterboxd_diary(username)\n",
        "  watched = await scrape_watched(username)\n",
        "  watchlist = await scrape_watchlist(username)\n",
        "  return last_20, watched, watchlist"
      ],
      "metadata": {
        "id": "uZFXWKskkmvW"
      },
      "execution_count": 128,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function makes scraped data usable"
      ],
      "metadata": {
        "id": "NfDPh4RSbJb7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_new_user_vector(scraped_user_data, fid_mapping, num_latent_factors=50):\n",
        "    new_user_df = pd.DataFrame(scraped_user_data)\n",
        "    new_user_df[\"fid_index\"] = new_user_df[\"fid\"].map(fid_mapping)\n",
        "    new_user_df = new_user_df.dropna(subset=[\"fid_index\"])\n",
        "\n",
        "    new_user_vector = np.zeros(num_latent_factors)\n",
        "\n",
        "    for _, row in new_user_df.iterrows():\n",
        "        try:\n",
        "            fid_index = int(row[\"fid_index\"])\n",
        "            new_user_vector[fid_index] = row[\"rating\"]\n",
        "        except IndexError:\n",
        "            pass\n",
        "\n",
        "    return new_user_vector"
      ],
      "metadata": {
        "id": "sayL9MvCjYu2"
      },
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function finds the most appropriate movie recommendations with a bias towards films in user's watchlist"
      ],
      "metadata": {
        "id": "-9evnWFNbQu3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_list_films(user_vector, latent_matrix, fid_mapping, watched_films, boost_fids=[], boost_factor=2):\n",
        "    similarities = np.dot(latent_matrix, user_vector)\n",
        "\n",
        "    for fid in boost_fids:\n",
        "        if fid in fid_mapping and fid_mapping[fid] < len(similarities):\n",
        "            similarities[fid_mapping[fid]] *= boost_factor\n",
        "\n",
        "    recommended_film_indices = np.argsort(similarities)[::-1]\n",
        "    recommended_fids = [\n",
        "        list(fid_mapping.keys())[i] for i in recommended_film_indices\n",
        "        if list(fid_mapping.keys())[i] not in watched_films\n",
        "    ][:10]\n",
        "    return recommended_fids"
      ],
      "metadata": {
        "id": "k7oS6uRZiWoM"
      },
      "execution_count": 130,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def recommend_films(username, latent_matrix, fid_mapping, boost_factor=1.5):\n",
        "  last_20, watched, watchlist = await scrap_lb(username)\n",
        "  new_user_vector = create_new_user_vector(last_20, fid_mapping)\n",
        "  return generate_list_films(new_user_vector, latent_matrix, fid_mapping, watched, watchlist, boost_factor)"
      ],
      "metadata": {
        "id": "RZJGlBsUjeaK"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This part prepares the model to be used. We will be using this dataset https://www.kaggle.com/datasets/samlearner/letterboxd-movie-ratings-data\n"
      ],
      "metadata": {
        "id": "hDImpATEb693"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_file = \"/content/drive/MyDrive/lb_recs/ratings_export.csv\"\n",
        "df = load_data(data_file)\n",
        "df, uid_mapping, fid_mapping = preprocess_data(df)\n",
        "num_users, num_films = len(uid_mapping), len(fid_mapping)\n",
        "\n",
        "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "rating_matrix = build_matrix(train_df, num_users, num_films)\n",
        "svd, latent_matrix = train_svd(rating_matrix)"
      ],
      "metadata": {
        "id": "VOItO6veiajK"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "FOUnCOB7cdPB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here you must input your letterboxd username"
      ],
      "metadata": {
        "id": "ni1oovxIb-Ll"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "username = input()\n",
        "recommendations = await recommend_films(username, latent_matrix, fid_mapping)\n",
        "print(f\"🎬 Recommendations: {recommendations}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V5h6P6tevR08",
        "outputId": "facc2969-0a35-47c6-d74d-311d1388bf28"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "anturgan\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The next part is to showcase the accurasy of the model. Average results:\n",
        "RMSE: 0.807\n",
        "MAE: 0.628"
      ],
      "metadata": {
        "id": "KNRffsJ9bctj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "y_true = []\n",
        "y_pred = []\n",
        "\n",
        "for _, row in test_df.iterrows():\n",
        "    uid_idx = row[\"uid_index\"]\n",
        "    fid_idx = row[\"fid_index\"]\n",
        "\n",
        "    if uid_idx < latent_matrix.shape[0] and fid_idx < latent_matrix.shape[1]:\n",
        "        pred_rating = np.dot(latent_matrix[uid_idx], svd.components_[:, fid_idx])\n",
        "        y_pred.append(pred_rating)\n",
        "        y_true.append(row[\"rating\"])\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
        "mae = mean_absolute_error(y_true, y_pred)\n",
        "\n",
        "print(f\"Model Performance:\")\n",
        "print(f\"RMSE: {rmse:.3f}\")\n",
        "print(f\"MAE: {mae:.3f}\")"
      ],
      "metadata": {
        "id": "nt1HbW2mEUWv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}